{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create Language Model from Scratch",
   "id": "beab85f2ab056c71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1 - Preprocess\n",
    "Preprocess the text by converting it to lowercase, removing punctuation, and filtering out rare or stop words."
   ],
   "id": "a87bf1ffef04a8a0"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-25T15:28:06.649110Z",
     "start_time": "2024-05-25T15:28:06.644237Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Replace with your dataset path\n",
    "#text = open(\"Henry_IV_Part_1_Yale.txt\").read().lower()\n",
    "text = open(\"quick_brown_fox.txt\").read().lower()\n",
    "print('Total characters: ', len(text))\n",
    "\n",
    "def preprocess(text):\n",
    "    t = text.replace(\"\\n\", \" \")  # Remove newlines\n",
    "    t = text.replace(\",\", \"\")  # Remove punctuation (customizable)\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "words = preprocess(text)\n",
    "print('Total words:', len(words))\n",
    "print('Words after preprocessing:', words)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters:  44\n",
      "Total words: 9\n",
      "Words after preprocessing: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2 - Build a Vocabulary\n",
    "\n",
    "Create a vocabulary of unique words in your dataset. Assign a unique integer ID to each word."
   ],
   "id": "d73fb00c76e30c4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:20:55.531739Z",
     "start_time": "2024-05-25T15:20:55.528791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Counter is a MultiSet data structure\n",
    "vocab = Counter(words)\n",
    "vocab_size = len(vocab)\n",
    "print('Vocab size:', vocab_size)\n",
    "print('Vocab: ', vocab)\n",
    "\n",
    "# Create a dictionary mapping words to unique IDs. In NLP tasks where you need to convert words to indices for vector representations or embeddings, using the positions of words in a vocabulary dictionary as a simple form of encoding.\n",
    "word2idx = {w: i for i, (w, _) in enumerate(vocab.items())}\n",
    "print('word2idx size:', len(word2idx))\n",
    "print('word2idx: ', word2idx)\n",
    "\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "print('idx2word size:', len(idx2word))\n",
    "print('idx2word: ', idx2word)"
   ],
   "id": "2989ab74662d9cda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8\n",
      "Vocab:  Counter({'the': 2, 'quick': 1, 'brown': 1, 'fox': 1, 'jumps': 1, 'over': 1, 'lazy': 1, 'dog.': 1})\n",
      "word2idx size: 8\n",
      "word2idx:  {'the': 0, 'quick': 1, 'brown': 2, 'fox': 3, 'jumps': 4, 'over': 5, 'lazy': 6, 'dog.': 7}\n",
      "idx2word size: 8\n",
      "idx2word:  {0: 'the', 1: 'quick', 2: 'brown', 3: 'fox', 4: 'jumps', 5: 'over', 6: 'lazy', 7: 'dog.'}\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3 - Implement a Tokenizer\n",
    "\n",
    "Write a function to convert text sequences into numerical sequences based on the vocabulary."
   ],
   "id": "75fc0c954d2bf3ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:20:55.536302Z",
     "start_time": "2024-05-25T15:20:55.533964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize(text, word2idx):\n",
    "  tokens = [word2idx[word] for word in text.split() if word in word2idx]\n",
    "  return tokens\n",
    "\n",
    "print(tokenize(text, word2idx))"
   ],
   "id": "8fb9726a2739a8e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 0, 6, 7]\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4 - Train/Build a Bigram Model\n",
    "\n",
    "A **Bigram** model predicts the next word based on the previous word. Train the model on your tokenized dataset. During training, the model adjusts its internal parameters to minimize the prediction error.\n",
    "\n",
    "If the input is **wireless speakers for tv**, the output will be the following:\n",
    "\n",
    "1. N=1 Unigram- Output- “wireless” , “speakers”, “for” , “tv”\n",
    "2. N=2 Bigram- Output- “wireless speakers”, “speakers for”, “for tv”\n",
    "3. N=3 Trigram – Output- “wireless speakers for”, “speakers for tv”"
   ],
   "id": "4ec2366918ee42d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:32:12.909778Z",
     "start_time": "2024-05-25T15:32:12.904122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_bigram_model(sequences):\n",
    "  print('Sequences; ', sequences)\n",
    "  # Initialize transition matrix with zeros\n",
    "  transition_matrix = np.zeros((vocab_size, vocab_size))\n",
    "  print('Transition matrix size:', transition_matrix.shape)\n",
    "  \n",
    "  for sequence in sequences:\n",
    "    for i in range(len(sequence) - 1):\n",
    "      transition_matrix[sequence[i], sequence[i + 1]] += 1\n",
    "\n",
    "  # Normalize each row to get probabilities\n",
    "  transition_matrix = transition_matrix / transition_matrix.sum(axis=1, keepdims=True)\n",
    "  return transition_matrix\n",
    "\n",
    "# Train the model on tokenized sequences\n",
    "# sequences = [tokenize(seq, word2idx) for seq in words]\n",
    "sequences = []\n",
    "for seq in words:\n",
    "    tokenized_seq = tokenize(seq, word2idx)\n",
    "    print(tokenized_seq)\n",
    "    sequences.append(tokenized_seq)  \n",
    "    \n",
    "model = build_bigram_model(sequences)\n",
    "\n",
    "print('Model', model)"
   ],
   "id": "bc7f2032f3ca6d42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[0]\n",
      "[6]\n",
      "[7]\n",
      "Sequences;  [[0], [1], [2], [3], [4], [5], [0], [6], [7]]\n",
      "Transition matrix size: (8, 8)\n",
      "Model [[nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6q/kx17lmxs2c30znt3t36xq8qh0000gn/T/ipykernel_58070/1090137822.py:12: RuntimeWarning: invalid value encountered in divide\n",
      "  transition_matrix = transition_matrix / transition_matrix.sum(axis=1, keepdims=True)\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 5 - Generate Text\n",
    "\n",
    "Once trained, use the model to generate new text by providing it with a seed sequence."
   ],
   "id": "a39d99cd7382a9e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:20:55.549036Z",
     "start_time": "2024-05-25T15:20:55.545999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text(model, start_word, max_length=100):\n",
    "  # Get start word index\n",
    "  start_idx = word2idx.get(start_word, None)\n",
    "  if start_idx is None:\n",
    "    print(\"Start word not in vocabulary\")\n",
    "    return\n",
    "\n",
    "  sequence = [start_idx]\n",
    "  for _ in range(max_length):\n",
    "    # Predict next word probability distribution\n",
    "    probs = model[sequence[-1]]\n",
    "\n",
    "    # Sample next word based on probabilities\n",
    "    next_idx = np.random.choice(range(vocab_size), p=probs)\n",
    "    sequence.append(next_idx)\n",
    "\n",
    "  # Convert numerical sequence back to text\n",
    "  text = \" \".join([idx2word[i] for i in sequence])\n",
    "  return text\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "start_text = \"the quick\"\n",
    "generated_text = generate_text(model, start_text)\n",
    "print(f\"Generated text: {generated_text}\")\n"
   ],
   "id": "787122b8a4ba6336",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word not in vocabulary\n",
      "Generated text: None\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:20:55.551134Z",
     "start_time": "2024-05-25T15:20:55.549867Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d2b22425866c46d8",
   "outputs": [],
   "execution_count": 39
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
